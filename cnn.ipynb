{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 2\n",
    "epochs = 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.datasets import  mnist \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def conv2d_vectorized(x, conv_filter, stride=1, padding=0):\n",
    "    # x: [H, W], conv_filter: [kH, kW]\n",
    "    x = x.unsqueeze(0).unsqueeze(0)  # [1,1,H,W]\n",
    "    x_unf = F.unfold(x, kernel_size=conv_filter.shape, stride=stride, padding=padding)\n",
    "    # x_unf: [1, kH*kW, out_H*out_W]\n",
    "    conv_flat = conv_filter.flatten().unsqueeze(1)  # [kH*kW, 1]\n",
    "    out = torch.matmul(conv_flat.T, x_unf)  # [1, out_H*out_W]\n",
    "    out_H = (x.shape[2] + 2*padding - conv_filter.shape[0]) // stride + 1\n",
    "    out_W = (x.shape[3] + 2*padding - conv_filter.shape[1]) // stride + 1\n",
    "    return out.view(1, out_H, out_W).squeeze(0)\n",
    "\n",
    "\n",
    "def conv2d(x, conv_filter, stride, padding):\n",
    "    H, W = x.shape \n",
    "\n",
    "    x_padded = torch.zeros(H+2*padding, W+2*padding, device=device)\n",
    "    x_padded[padding:H+padding, padding:W+padding] = x \n",
    "\n",
    "    kH, kW = conv_filter.shape\n",
    "    \n",
    "    out_H = (H + 2*padding - kH)//stride + 1 \n",
    "    out_W = (W + 2*padding - kW)//stride + 1 \n",
    "\n",
    "    output_map = torch.zeros(out_H, out_W, device=device)\n",
    "\n",
    "    for i in range(0, out_H*stride, stride):\n",
    "        for j in range(0, out_W*stride, stride):\n",
    "            output_map[i//stride, j//stride] = torch.sum((x_padded[i:i+kH, j:j+kW] * conv_filter))\n",
    "    return output_map\n",
    "\n",
    "\n",
    "class ConvolutionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, padding, stride, filter_size):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.filters = nn.Parameter(torch.randn(number_of_filter, filter_size, filter_size) * 0.1)\n",
    "        self.filters = nn.Parameter(torch.randn(output_channels, input_channels, filter_size, filter_size, device=device) * 0.1)\n",
    "\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.filter_size = filter_size \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  ### [B, C, H, W]\n",
    "\n",
    "        filters_flat = self.filters.view(self.output_channels, -1) \n",
    "\n",
    "        x_unf = F.unfold(x, kernel_size=self.filter_size, padding=self.padding, stride=self.stride)\n",
    "\n",
    "        filters_flat_exp = filters_flat.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "\n",
    "        out = torch.bmm(filters_flat_exp, x_unf)\n",
    "        out_H = (H + 2*self.padding - self.filter_size)//self.stride + 1\n",
    "        out_W = (W + 2*self.padding - self.filter_size)//self.stride + 1\n",
    "        out = out.view(B, self.number_of_filter, out_H, out_W)\n",
    "        return out\n",
    "\n",
    "\n",
    "        # output = [] \n",
    "        \n",
    "        # for b in range(B):\n",
    "        #     feature_map = []\n",
    "        #     for j in range(self.number_of_filter):\n",
    "        #         #output_conv2d = conv2d(x[b, 0], self.filters[j].to(device),  self.stride, self.padding)\n",
    "        #         output_conv2d = conv2d_vectorized(x[b, 0], self.filters[j].to(device),  self.stride, self.padding)\n",
    "        #         feature_map.append(output_conv2d) ##[1, out_H, out_W]\n",
    "        #     output.append(torch.stack(feature_map)) ##[F, out_H, out_W]\n",
    "        # return torch.stack(output) ##[B, F, out_H, out_W]\n",
    "    \n",
    "\n",
    "class CNNMOdel(nn.Module):\n",
    "\n",
    "    def __init__(self, classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = ConvolutionLayer(input_channels=1, output_channels=2 padding=1, stride=1, filter_size=3)\n",
    "        self.conv2 = ConvolutionLayer(input_channels=2, output_channels=2, padding=1, stride=1, filter_size=3)\n",
    "        num_features = 2*28*28\n",
    "        self.classifier = nn.Linear(num_features, classes, device=device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten [B, 8*28*28]\n",
    "        x = self.classifier(x) \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images[:10]\n",
    "train_labels = train_labels[:10]\n",
    "test_images = test_images[:10]\n",
    "test_labels = test_labels[:10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert to float tensors and normalize\n",
    "train_images = torch.tensor(train_images, dtype=torch.bfloat16) / 255.0\n",
    "test_images = torch.tensor(test_images, dtype=torch.bfloat16) / 255.0\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Add channel dimension: (N, C, H, W)\n",
    "train_images = train_images.unsqueeze(1)  # (N, 1, 28, 28)\n",
    "test_images = test_images.unsqueeze(1)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = torch.utils.data.TensorDataset(train_images, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Model\n",
    "model = CNNMOdel(classes=10)\n",
    "model  = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "total_steps = epochs * train_loader.__len__()\n",
    "\n",
    "steps = 0 \n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        steps+=1\n",
    "\n",
    "        if steps%5==0:\n",
    "            print (f\"steps {steps} Loss {loss.item()}\")\n",
    "            \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from torchvision.datasets import CocoCaptions, CocoDetection\n",
    "from torchvision import transforms\n",
    "import torch \n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset_cocooptions = CocoCaptions(\n",
    "    root='train2017',\n",
    "    annFile='annotations/captions_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset_detection = CocoDetection(\n",
    "    root='train2017',\n",
    "    annFile='annotations/instances_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "N = 1000\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "train_dataset_cocooptions = Subset(train_dataset_cocooptions, range(N))\n",
    "train_dataset_detection = Subset(train_dataset_detection, range(N))\n",
    "\n",
    "\n",
    "all_captions = \"\\n\".join([caption for captions_list in train_dataset_cocooptions for caption in captions_list[1]])\n",
    "all_words = list(all_captions.split(\" \"))\n",
    "\n",
    "\n",
    "\n",
    "counter = Counter()\n",
    "for word in all_words:\n",
    "    counter[word]+=1\n",
    "\n",
    "vocab = [word for word, cnt in counter.items() if cnt>5]\n",
    "vocab +=[\"UNK\", \"<START>\", \"<END>\", \"<PAD>\"]\n",
    "\n",
    "\n",
    "word2idx =  {item:i for i, item in enumerate(vocab)}\n",
    "idx2word =  {i:item for i, item in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def encode(stri):\n",
    "    all_tensor = [word2idx.get(word, word2idx[\"UNK\"]) for word in stri.split(\" \")]\n",
    "    return all_tensor \n",
    "\n",
    "def decode(input_tensor):\n",
    "    return [idx2word[each] for each in input_tensor]\n",
    "    \n",
    "\n",
    "class DataLoaderLite(Dataset):\n",
    "\n",
    "    def __init__(self, train_dataset_cocooptions, caption_length=50):\n",
    "        self.train_dataset_cocooptions = train_dataset_cocooptions\n",
    "        self.caption_length = caption_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_dataset_cocooptions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_tensor, image_captions = self.train_dataset_cocooptions[idx]\n",
    "        caption = \"<START> \" + image_captions[0] + \" <END>\"\n",
    "        caption_tensor = encode(caption)\n",
    "\n",
    "        if len(caption_tensor) < self.caption_length:\n",
    "            caption_tensor += [word2idx[\"<PAD>\"]] * (self.caption_length - len(caption_tensor))\n",
    "\n",
    "        else:\n",
    "            caption_tensor = caption_tensor[:self.caption_length]\n",
    "        return image_tensor, torch.tensor(caption_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f69fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "from torch.functional import F \n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from torchvision.datasets import CocoCaptions, CocoDetection\n",
    "from torchvision import transforms\n",
    "import torch \n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "special_tokens = {\"additional_special_tokens\": [\"<START>\", \"<END>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(\"<PAD>\")\n",
    "start_token_id = tokenizer.convert_tokens_to_ids(\"<START>\") \n",
    "end_token_id = tokenizer.convert_tokens_to_ids(\"<END>\") \n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset_cocooptions = CocoCaptions(\n",
    "    root='train2017',\n",
    "    annFile='annotations/captions_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset_detection = CocoDetection(\n",
    "    root='train2017',\n",
    "    annFile='annotations/instances_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "N = 1000\n",
    "\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "train_dataset_cocooptions = Subset(train_dataset_cocooptions, range(N))\n",
    "train_dataset_detection = Subset(train_dataset_detection, range(N))\n",
    "\n",
    "\n",
    "# all_captions = \"\\n\".join([caption for captions_list in train_dataset_cocooptions for caption in captions_list[1]])\n",
    "# all_words = list(all_captions.split(\" \"))\n",
    "\n",
    "\n",
    "\n",
    "# counter = Counter()\n",
    "# for word in all_words:\n",
    "#     counter[word]+=1\n",
    "\n",
    "# vocab = [word for word, cnt in counter.items() if cnt>5]\n",
    "# vocab +=[\"UNK\", \"<START>\", \"<END>\", \"<PAD>\"]\n",
    "\n",
    "\n",
    "# word2idx =  {item:i for i, item in enumerate(vocab)}\n",
    "# idx2word =  {i:item for i, item in enumerate(vocab)}\n",
    "\n",
    "\n",
    "# def encode(stri):\n",
    "#     all_tensor = [word2idx.get(word, word2idx[\"UNK\"]) for word in stri.split(\" \")]\n",
    "#     return all_tensor \n",
    "\n",
    "# def decode(input_tensor):\n",
    "#     return [idx2word[each] for each in input_tensor]\n",
    "\n",
    "\n",
    "class DataLoaderLite(Dataset):\n",
    "    def __init__(self, train_dataset_cocooptions, caption_length=50, tokenizer=tokenizer):\n",
    "        self.train_dataset_cocooptions = train_dataset_cocooptions\n",
    "        self.caption_length = caption_length\n",
    "        self.tokenizer = tokenizer \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_dataset_cocooptions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_tensor, image_captions = self.train_dataset_cocooptions[idx]\n",
    "\n",
    "        # prepend <START>, append <END>\n",
    "        caption = \"<START> \" + image_captions[0] + \" <END>\"\n",
    "\n",
    "        # tokenize with GPT2 tokenizer\n",
    "        tokens = self.tokenizer(\n",
    "            caption,\n",
    "            max_length=self.caption_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return image_tensor, tokens[\"input_ids\"].squeeze(0), tokens[\"attention_mask\"].squeeze(0)\n",
    "    \n",
    "    # def __getitem__(self, idx):\n",
    "    #     image_tensor, image_captions = self.train_dataset_cocooptions[idx]\n",
    "\n",
    "    #     # Prepend <IMG> + <START>, Append <END>\n",
    "    #     caption = \"<IMG> <START> \" + image_captions[0] + \" <END>\"\n",
    "    #     caption_tensor = encode(caption)\n",
    "\n",
    "    #     if len(caption_tensor) < self.caption_length:\n",
    "    #         caption_tensor += [word2idx[\"<PAD>\"]] * (self.caption_length - len(caption_tensor))\n",
    "    #     else:\n",
    "    #         caption_tensor = caption_tensor[:self.caption_length]\n",
    "\n",
    "    #     return image_tensor, torch.tensor(caption_tensor)\n",
    "\n",
    "    \n",
    "\n",
    "# class DataLoaderLite(Dataset):\n",
    "\n",
    "#     def __init__(self, train_dataset_cocooptions, caption_length=50):\n",
    "#         self.train_dataset_cocooptions = train_dataset_cocooptions\n",
    "#         self.caption_length = caption_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.train_dataset_cocooptions)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         image_tensor, image_captions = self.train_dataset_cocooptions[idx]\n",
    "#         caption = \"<START> \" + image_captions[0] + \" <END>\"\n",
    "#         caption_tensor = encode(caption)\n",
    "\n",
    "#         if len(caption_tensor) < self.caption_length:\n",
    "#             caption_tensor += [word2idx[\"<PAD>\"]] * (self.caption_length - len(caption_tensor))\n",
    "\n",
    "#         else:\n",
    "#             caption_tensor = caption_tensor[:self.caption_length]\n",
    "#         return image_tensor, torch.tensor(caption_tensor)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "def conv2d(x, kernel, stride, padding):\n",
    "    H, W = x.shape\n",
    "    device = x.device \n",
    "    x_padded = torch.zeros((H+2*padding, W+2*padding), device=device)\n",
    "    x_padded[padding:H+padding, padding:W+padding] = x \n",
    "\n",
    "    kH, kW = kernel.shape \n",
    "    out_H = (H+2*padding-kH)//stride +1\n",
    "    out_W = (W+2*padding-kW)//stride +1\n",
    "\n",
    "    feature_map = torch.zeros((out_H, out_W), device=device)\n",
    "\n",
    "    for i in range(0, (H+2*padding-kH+1), stride):\n",
    "        for j in range(0, (W+2*padding-kW+1), stride):\n",
    "            region = x_padded[i:i+kH, j:j+kW] \n",
    "            feature_map[i//stride, j//stride] = torch.sum(kernel.to(region.device) * region)\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "\n",
    "class ConvolutionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, padding, stride, kernel_size):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.padding = padding\n",
    "        self.stride = stride \n",
    "        self.kernel_size = kernel_size \n",
    "        self.kernel = nn.Parameter(torch.randn(self.output_channels, self.input_channels, self.kernel_size, self.kernel_size))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        output = []\n",
    "        for batch in range(B):\n",
    "            output_feature_map = []\n",
    "            for each_output_channel in range(self.output_channels):\n",
    "                feature_map = torch.zeros(((H+2*self.padding-self.kernel_size)//self.stride +1, (W+2*self.padding-self.kernel_size)//self.stride +1), device=device)\n",
    "\n",
    "                for each_input_channel in range(self.input_channels):\n",
    "                    feature_map += conv2d(x[batch, each_input_channel], self.kernel[each_output_channel, each_input_channel], self.stride, self.padding)\n",
    "                output_feature_map.append(feature_map)\n",
    "            output.append(torch.stack(output_feature_map))\n",
    "        return torch.stack(output)\n",
    "    \n",
    "\n",
    "\n",
    "class ResnetGPT2Wrapper(nn.Module):\n",
    "    def __init__(self, gpt_decoder, embed_size, vocab_size, num_img_tokens=5):\n",
    "        super().__init__()\n",
    "        self.gpt_decoder = gpt_decoder\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_img_tokens = num_img_tokens\n",
    "\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_size, num_heads=4, batch_first=True)\n",
    "        self.key_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.value_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.query_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.layernorm = nn.LayerNorm(embed_size, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.img_queries = nn.Parameter(torch.randn(num_img_tokens, embed_size) * 0.01)\n",
    "\n",
    "\n",
    "    def forward(self, img_features, captions_tensor, attention_mask=None):\n",
    "        img_features = img_features.float()\n",
    "         # 1. Token embeddings from GPT2\n",
    "        tok_embeds = self.gpt_decoder.transformer.wte(captions_tensor)  # (B, T, D)\n",
    "\n",
    "        B = tok_embeds.shape[0]\n",
    "\n",
    "        queries = self.img_queries.unsqueeze(0).expand(B, -1, -1)  # (B, num_img_tokens, D)\n",
    "\n",
    "\n",
    "        B, T, D = tok_embeds.shape\n",
    "        N = img_features.shape[1]\n",
    "\n",
    "        k = self.key_proj(img_features)              # (B, N, D)\n",
    "        v = self.value_proj(img_features)            # (B, N, D)\n",
    "        \n",
    "        enriched, _ = self.mha(self.query_proj(queries), k, v)  # (B, M, D)\n",
    "\n",
    "        enriched = self.layernorm(queries + enriched) \n",
    "\n",
    "        fused = torch.cat([enriched, tok_embeds], dim=1)  # (B, M+T, D)\n",
    "\n",
    "        # query = self.query_proj(tok_embeds)\n",
    "        # keys  = self.key_proj(img_features)\n",
    "        # values = self.value_proj(img_features)\n",
    "        # enriched, attn_weights = self.mha(query, keys, values)\n",
    "\n",
    "        enriched = self.dropout(fused)\n",
    "\n",
    "\n",
    "        # enriched = enriched + tok_embeds  # residual connection\n",
    "\n",
    "        inputs_embeds = enriched[:, :-1, :].contiguous()\n",
    "        labels = captions_tensor[:, 1:].contiguous()\n",
    "\n",
    "\n",
    "        pad_for_img = torch.full((B, self.num_img_tokens, ), pad_token_id, dtype=torch.long, device=labels.device)\n",
    "\n",
    "        labels = torch.cat([pad_for_img, labels], dim=1)   # (B, M + T - 1)\n",
    "\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            img_mask = torch.ones(B, self.num_img_tokens, device=attention_mask.device)\n",
    "            attention_mask = torch.cat([img_mask, attention_mask], dim=1)\n",
    "            attention_mask = attention_mask[:, :-1].contiguous()\n",
    "\n",
    "        outputs = self.gpt_decoder(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        return outputs.logits, outputs.loss \n",
    "\n",
    "    \n",
    "class LSTMEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm_layer = nn.LSTM(2*embed_size, hidden_size=hidden_size, batch_first=True, num_layers=3)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_size, num_heads=4, batch_first=True)\n",
    "        self.key_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.value_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.query_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embedding_layer(captions[:, :-1])  # teacher forcing\n",
    "        # features = features.unsqueeze(1)  # (B, 1, embed_size)\n",
    "\n",
    "        \n",
    "        query = self.query_proj(embeddings)\n",
    "\n",
    "        keys  = self.key_proj(features) \n",
    "\n",
    "        values = self.value_proj(features)\n",
    "\n",
    "        # keys = keys.unsqueeze(1)\n",
    "        # values = values.unsqueeze(1)\n",
    "\n",
    "\n",
    "        attn_out, attn_weights = self.mha(query, keys, values)\n",
    "\n",
    "\n",
    "        attn_out = torch.cat((embeddings, attn_out), dim=-1)\n",
    "\n",
    "\n",
    "        # print (f\"==== attn_weights\", attn_weights.shape)\n",
    "        # LLLL\n",
    "\n",
    "        # inputs = torch.cat((attn_out, embeddings), dim=1)\n",
    "        outputs, _ = self.lstm_layer(attn_out)\n",
    "\n",
    "\n",
    "        outputs = self.fc(outputs)  # (B, T, vocab_size)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_h, grid_w):\n",
    "    \"\"\"Return 2D sine-cosine positional embeddings\"\"\"\n",
    "    grid_y = torch.arange(grid_h, dtype=torch.bfloat16)\n",
    "    grid_x = torch.arange(grid_w, dtype=torch.bfloat16)\n",
    "    grid = torch.meshgrid(grid_y, grid_x, indexing='ij')  # (H, W)\n",
    "    grid = torch.stack(grid, dim=-1)  # (H, W, 2)\n",
    "\n",
    "    # flatten\n",
    "    grid = grid.reshape(-1, 2)  # (H*W, 2)\n",
    "\n",
    "    # compute embeddings\n",
    "    pos_emb = []\n",
    "    for dim in range(embed_dim // 2):\n",
    "        div_term = 10000 ** (2 * (dim // 2) / embed_dim)\n",
    "        pos_emb.append(torch.sin(grid / div_term) if dim % 2 == 0 else torch.cos(grid / div_term))\n",
    "    pos_emb = torch.cat(pos_emb, dim=1)  # (H*W, embed_dim)\n",
    "    return pos_emb\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResnetEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, freeze_until_layer=5):\n",
    "        super().__init__()\n",
    "        # load pretrained ResNet\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]  # remove the last fc layer\n",
    "        self.backbone = nn.Sequential(*modules)\n",
    "\n",
    "        # freeze layers\n",
    "        child_counter = 0\n",
    "        for child in self.backbone.children():\n",
    "            if child_counter < freeze_until_layer:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            child_counter += 1\n",
    "       \n",
    "\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, 3, 224, 224) -> (B, 2048, H/32, W/32)\n",
    "        feats = self.backbone(x)\n",
    "        B, C, H, W = feats.shape\n",
    "        feats = feats.view(B, C, -1).permute(0, 2, 1)  # (B, H*W, C)\n",
    "        feats = self.fc(feats)  # (B, H*W, embed_size)\n",
    "        return feats\n",
    "\n",
    "\n",
    "checkpoint_path = \"checkpoint.pth\"\n",
    "\n",
    "def save_model(image_encoder, caption_encoder):\n",
    "    model_dict = {\n",
    "        \"image_encoder_state\": image_encoder.state_dict(),\n",
    "        \"caption_encoder_state\": caption_encoder.state_dict(),\n",
    "        \"image_encoder_class\": image_encoder.__class__,\n",
    "        \"caption_encoder_class\": caption_encoder.__class__,\n",
    "        \"image_encoder_args\": image_encoder.args if hasattr(image_encoder, \"args\") else (),\n",
    "        \"caption_encoder_args\": caption_encoder.args if hasattr(caption_encoder, \"args\") else (),\n",
    "    }\n",
    "    torch.save(model_dict, checkpoint_path)\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    model_dict = torch.load(checkpoint_path, map_location=\"mps\")\n",
    "\n",
    "    image_encoder = model_dict[\"image_encoder_class\"](*model_dict[\"image_encoder_args\"])\n",
    "    caption_encoder = model_dict[\"caption_encoder_class\"](*model_dict[\"caption_encoder_args\"])\n",
    "\n",
    "    image_encoder.load_state_dict(model_dict[\"image_encoder_state\"])\n",
    "    caption_encoder.load_state_dict(model_dict[\"caption_encoder_state\"])\n",
    "\n",
    "    return image_encoder, caption_encoder\n",
    "\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, input_shape):\n",
    "        super().__init__()\n",
    "   \n",
    "        # More filters + strides to reduce spatial dims\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)  # 224 -> 112\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1) # 112 -> 56\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1) # 56 -> 28\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1) # 28 -> 14\n",
    "\n",
    "        pos_emb = get_2d_sincos_pos_embed(embed_size, 14, 14)  # (196, embed_size)\n",
    "        self.register_buffer(\"pos_embed\", pos_emb.unsqueeze(0))  # (1, 196, embed_size)\n",
    "\n",
    "\n",
    "        self.to(device)\n",
    "        with torch.no_grad():\n",
    "            B, C, H, W = input_shape[:]\n",
    "            x_dummy = torch.randn((B, C, H, W), device=device)\n",
    "            x_dummy = self.conv1(x_dummy)\n",
    "            x_dummy = self.conv2(x_dummy)\n",
    "            x_dummy = self.conv3(x_dummy)\n",
    "            x_dummy = self.conv4(x_dummy)\n",
    "            B, C, H, W = x_dummy.shape \n",
    "            del x_dummy\n",
    "            torch.mps.empty_cache()  # if using MPS\n",
    "            import gc; gc.collect()\n",
    "        self.fc = nn.Linear(C, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x) \n",
    "        x = self.conv2(x) \n",
    "        x = F.relu(x) \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.reshape(B, C, H*W)\n",
    "        x = x.permute(0, 2, 1)  # (B, H*W, C)\n",
    "        # x_embed = self.fc(x)  # (B, H*W, embed_size\n",
    "        B, N, C = x.shape\n",
    "        x_embed = self.fc(x)   # (B, N, embed_size) \n",
    "        x_embed = x_embed + self.pos_embed[:, :N, :].to(x_embed.device)  # add positional embedding\n",
    "        return x_embed  \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# # caption_encoder = LSTMEncoder(embed_size, hidden_size, vocab_size)\n",
    "# from transformers import GPT2LMHeadModel\n",
    "# gpt_decoder = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# gpt_decoder.resize_token_embeddings(gpt_decoder.get_input_embeddings().num_embeddings + 2)  # Example: add 3 tokens\n",
    "# vocab_size = gpt_decoder.get_input_embeddings().num_embeddings\n",
    "\n",
    "\n",
    "\n",
    "# gpt_hidden_size = gpt_decoder.config.hidden_size\n",
    "# embed_size = gpt_hidden_size  # to match GPT2 hidden size\n",
    "# hidden_size = gpt_hidden_size\n",
    "# batch_size = 4\n",
    "# input_channels = 3  \n",
    "# image_h, image_w = 224, 224\n",
    "# steps = 0\n",
    "# epochs = 1\n",
    "# lr = 1e-5\n",
    "# accumulation_steps = 4  # simulate batch_size * 2\n",
    "\n",
    " \n",
    "\n",
    "gpt_decoder = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt_decoder.resize_token_embeddings(gpt_decoder.get_input_embeddings().num_embeddings + 2)  # Example: add 3 tokens\n",
    "\n",
    "\n",
    "checkpoint_path = \"checkpoint.pth\"\n",
    "device = \"mps\"\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    gpt_hidden_size = gpt_decoder.config.hidden_size\n",
    "    embed_size = gpt_hidden_size  # to match GPT2 hidden size\n",
    "    hidden_size = gpt_hidden_size\n",
    "    batch_size = 4\n",
    "    input_channels = 3  \n",
    "    image_h, image_w = 224, 224\n",
    "    steps = 0\n",
    "    epochs = 1\n",
    "    lr = 1e-5\n",
    "    accumulation_steps = 4  # simulate batch_size * 2\n",
    "    vocab_size = gpt_decoder.get_input_embeddings().num_embedding\n",
    "\n",
    "\n",
    "train_dataset_cocooptions=DataLoaderLite(train_dataset_cocooptions, caption_length=50, tokenizer=tokenizer)           \n",
    "train_dataloader = DataLoader(train_dataset_cocooptions, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "total_steps = len(train_dataloader)  * epochs\n",
    "import math \n",
    "\n",
    "formatted_str = f\"Training details vocab size {vocab_size} batch size {batch_size} image size {image_h}x{image_w}\"\n",
    "formatted_str+= f\" total steps {total_steps} epochs {epochs}\"\n",
    "formatted_str+= f\"Max loss {math.log(vocab_size)}\"\n",
    "formatted_str+= f\"Perplexity {math.exp(math.log(vocab_size))}\"\n",
    "\n",
    "\n",
    "print (formatted_str)\n",
    "scaler = GradScaler()\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "import gc; gc.collect()\n",
    "\n",
    "#encoder_model = CNNEncoder(embed_size, [batch_size, input_channels, image_h, image_w])\n",
    "\n",
    "encoder_model = ResnetEncoder(embed_size)\n",
    "encoder_model = encoder_model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "caption_encoder = ResnetGPT2Wrapper(gpt_decoder, embed_size, vocab_size)\n",
    "\n",
    "caption_encoder = caption_encoder.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_params = list([param for param in encoder_model.parameters() if param.requires_grad]) + list(caption_encoder.parameters())\n",
    "\n",
    "# print (\"Trainable parameters in encoder model:\")\n",
    "# print (sum(p.numel() for p in all_params if p.requires_grad))\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(all_params, lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps/accumulation_steps, eta_min=1e-6)\n",
    "\n",
    "import time \n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # print (f\"Starting epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2] # [B, 3, 224, 224], [B, T], [B, T] \n",
    "        image_tensor, caption_tensor, attention_mask = image_tensor.to(device), caption_tensor.to(device), attention_mask.to(device)\n",
    "        B, C, H, W = image_tensor.shape\n",
    "\n",
    "        global_step = epoch * len(train_dataloader) + step + 1\n",
    "\n",
    "        # print (f\"Step {step+1}/{len(train_dataloader)} Global step {global_step}/{total_steps}\")\n",
    "     \n",
    "\n",
    "        with torch.autocast(\"mps\", enabled=True, dtype=torch.bfloat16):\n",
    "\n",
    "            # print (\"Running encoder model\")\n",
    "            # print(\"current allocated memory:\", torch.mps.current_allocated_memory() / 1e9, \"GB\")\n",
    "            # print(\"driver allocated memory:\", torch.mps.driver_allocated_memory() / 1e9, \"GB\")\n",
    "\n",
    "            x_embed = encoder_model(image_tensor) # (B, N, embed_size) \n",
    "\n",
    "            # print(\"current allocated memory:\", torch.mps.current_allocated_memory() / 1e9, \"GB\")\n",
    "            # print(\"driver allocated memory:\", torch.mps.driver_allocated_memory() / 1e9, \"GB\")\n",
    "\n",
    "\n",
    "            #x_caption = caption_encoder(x_embed, caption_tensor)\n",
    "\n",
    "\n",
    "            logits, caption_loss  = caption_encoder(x_embed, caption_tensor, attention_mask)  # (B, T-1, vocab_size)\n",
    "\n",
    "            # B, T, C = logits.shape\n",
    "            # preds = logits.reshape(B*T, C)\n",
    "            # targets = caption_tensor[:, 1:].reshape(-1)\n",
    "\n",
    "            # print (f\" prediction {preds.shape} targets: {targets.shape}\")\n",
    "            \n",
    "            # caption_loss = loss_fn(preds, targets)\n",
    "            loss = caption_loss / accumulation_steps  \n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        all_params = list([param for param in encoder_model.parameters() if param.requires_grad]) + list(caption_encoder.parameters())\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, max_norm=5.0)\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "          # estimate remaining time every 100 steps\n",
    "        if global_step % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            steps_per_sec = global_step / elapsed\n",
    "            remaining_steps = total_steps - global_step\n",
    "            est_remaining = remaining_steps / steps_per_sec\n",
    "            est_total = total_steps / steps_per_sec\n",
    "\n",
    "            print(f\"epoch {epoch+1}/{epochs} step {step}/{len(train_dataloader)} \"\n",
    "                  f\"Loss: {loss.item()*accumulation_steps:.4f} | \"\n",
    "                  f\"Elapsed: {elapsed/60:.2f} min | \"\n",
    "                  f\"ETA: {est_remaining/60:.2f} min | \"\n",
    "                  f\"Total est: {est_total/60:.2f} min | \"\n",
    "                  f\"Memory: {torch.mps.current_allocated_memory() / 1e9:.2f} GB , \\ {torch.mps.driver_allocated_memory() / 1e9:.2f} GB | \"\n",
    "                  f\"Perplexity {math.exp(loss.item()*accumulation_steps):.2f}\"\n",
    "                  )\n",
    "            \n",
    "            save_model(image_encoder=encoder_model, caption_encoder=caption_encoder)\n",
    "\n",
    "    if (step + 1) % accumulation_steps != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, 5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        # if step % 100 == 0:\n",
    "        #     print(f\" epoch {epoch+1}/{epochs} step {step}/{total_steps} Loss: {loss.item()}\")\n",
    "\n",
    "        # if step % 1 == 0:\n",
    "        #     print(\"current allocated memory:\", torch.mps.current_allocated_memory() / 1e9, \"GB\")\n",
    "        #     print(\"driver allocated memory:\", torch.mps.driver_allocated_memory() / 1e9, \"GB\")\n",
    "\n",
    "\n",
    "    del image_tensor, caption_tensor, x_embed, logits\n",
    "    torch.mps.empty_cache()\n",
    "    import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "##### tokenizer #####\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "decoder_model_name = \"Falcon3-1B-Base\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder_model_name)\n",
    "# If no pad token, use eos\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "   \n",
    "\n",
    "special_tokens = {\"additional_special_tokens\": [\"<START>\", \"<END>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "start_token_id = tokenizer.convert_tokens_to_ids(\"<START>\") \n",
    "end_token_id = tokenizer.convert_tokens_to_ids(\"<END>\") \n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "\n",
    "class GPT2WithCross(GPT2LMHeadModel):\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        attention_mask=None,\n",
    "        img_feats=None,\n",
    "        labels=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if input_ids is not None:\n",
    "            inputs_embeds = self.transformer.wte(input_ids)\n",
    "\n",
    "        # Run through transformer, passing img_feats to each block\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            hidden_states = block(hidden_states, img_feats=img_feats, attention_mask=attention_mask)[0]\n",
    "\n",
    "        # Layer norm\n",
    "        hidden_states = self.transformer.ln_f(hidden_states)\n",
    "\n",
    "\n",
    "        # LM head\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift for language modeling loss\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5150f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn \n",
    "\n",
    "### Image Encoder Block #### \n",
    "\n",
    "class ResnetEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, freeze_until_layer=6):\n",
    "        super().__init__()\n",
    "        # load pretrained ResNet\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]  # remove the last fc layer\n",
    "        self.backbone = nn.Sequential(*modules)\n",
    "\n",
    "        # freeze layers\n",
    "        child_counter = 0\n",
    "        for child in self.backbone.children():\n",
    "            if child_counter < freeze_until_layer:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            child_counter += 1\n",
    "       \n",
    "\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, 3, 224, 224) -> (B, 2048, H/32, W/32)\n",
    "        feats = self.backbone(x)\n",
    "        B, C, H, W = feats.shape\n",
    "        feats = feats.view(B, C, -1).permute(0, 2, 1)  # (B, H*W, C)\n",
    "        feats = self.fc(feats)  # (B, H*W, embed_size)\n",
    "        return feats.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f8bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel\n",
    "import torch.nn as nn \n",
    "import os \n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "\n",
    "HF_TOKEN = \"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\"\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "class CLIPEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, model_name=\"openai/clip-vit-base-patch32\", freeze_vision=True):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(model_name)\n",
    "        self.vision = self.clip.vision_model  # Vision tower (ViT)\n",
    "\n",
    "        if freeze_vision:\n",
    "            for param in self.vision.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.proj = nn.Linear(self.vision.config.hidden_size, embed_size)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, 3, H, W), raw images expected to be normalized \n",
    "        Returns: (B, num_patches, embed_size)\n",
    "        \"\"\"\n",
    "        # CLIP’s vision forward gives hidden states for each patch\n",
    "        outputs = self.vision(pixel_values=x, output_hidden_states=False)\n",
    "        feats = outputs.last_hidden_state  # (B, num_patches+1, D) \n",
    "\n",
    "        # Optionally drop CLS token (index 0), keep patch tokens\n",
    "        feats = feats[:, 1:, :]  # (B, num_patches, D)\n",
    "\n",
    "        feats = self.proj(feats)  # (B, num_patches, embed_size)\n",
    "        return feats.to(torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "### Decoder Block ####\n",
    "\n",
    "class ResnetGPT2Wrapper(nn.Module):\n",
    "    def __init__(self, gpt_decoder, embed_size, vocab_size, num_img_tokens=32):\n",
    "        super().__init__()\n",
    "        self.gpt_decoder = gpt_decoder\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_img_tokens = num_img_tokens\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_size, num_heads=4, batch_first=True)\n",
    "        self.key_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.value_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.query_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.layernorm = nn.LayerNorm(embed_size, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_size, eps=1e-6)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.img_queries = nn.Parameter(torch.randn(num_img_tokens, embed_size) * 0.01)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_size, 4 * embed_size, dtype=torch.bfloat16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_size, embed_size, dtype=torch.bfloat16),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, img_features, captions_tensor, attention_mask=None, mode=\"train\"):\n",
    "        img_features = img_features\n",
    "         # 1. Token embeddings from GPT2\n",
    "\n",
    "\n",
    "        #tok_embeds = self.gpt_decoder.transformer.wte(captions_tensor)  # (B, T, D)\n",
    "        tok_embeds = self.gpt_decoder.get_input_embeddings()(captions_tensor)\n",
    "\n",
    "        B = tok_embeds.shape[0]\n",
    "\n",
    "        queries = self.img_queries.unsqueeze(0).expand(B, -1, -1)  # (B, num_img_tokens, D)\n",
    "\n",
    "\n",
    "        B, T, D = tok_embeds.shape\n",
    "        N = img_features.shape[1]\n",
    "\n",
    "        k = self.key_proj(img_features)              # (B, N, D)\n",
    "        v = self.value_proj(img_features)            # (B, N, D)\n",
    "        \n",
    "        enriched, _ = self.mha(self.query_proj(queries), k, v)  # (B, M, D)\n",
    "\n",
    "        enriched = self.layernorm(queries + enriched) \n",
    "\n",
    "        # --- MLP Block ---\n",
    "        mlp_out = self.mlp(enriched)\n",
    "        enriched = self.layernorm2(enriched + mlp_out)  #\n",
    "\n",
    "        fused = torch.cat([enriched, tok_embeds], dim=1)  # (B, M+T, D)\n",
    "\n",
    "\n",
    "        enriched = self.dropout(fused)\n",
    "\n",
    "\n",
    "        # enriched = enriched + tok_embeds  # residual connection\n",
    "\n",
    "        inputs_embeds = enriched[:, :-1, :].contiguous()\n",
    "\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            img_mask = torch.ones(B, self.num_img_tokens, device=attention_mask.device)\n",
    "            attention_mask = torch.cat([img_mask, attention_mask], dim=1)\n",
    "            attention_mask = attention_mask[:, :-1].contiguous()\n",
    "            # attention_mask = attention_mask[:, None, None, :].to(dtype=inputs_embeds.dtype)  # (B, 1, 1, seq_len)\n",
    "\n",
    "\n",
    "        labels = None \n",
    "        if mode=='train':\n",
    "            labels = captions_tensor[:, 1:].contiguous()\n",
    "            pad_for_img = torch.full((B, self.num_img_tokens, ), pad_token_id, dtype=torch.long, device=labels.device)\n",
    "            labels = torch.cat([pad_for_img, labels], dim=1)   # (B, M + T - 1)\n",
    "\n",
    "\n",
    "        outputs = self.gpt_decoder(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "\n",
    "        # outputs = self.gpt_decoder(\n",
    "        #     inputs_embeds=inputs_embeds,\n",
    "        #     attention_mask=attention_mask,\n",
    "        #     img_feats=img_features, \n",
    "        #     labels=labels\n",
    "        # )\n",
    "        return outputs.logits, outputs.loss \n",
    "        #return outputs[\"logits\"], outputs[\"loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_fast=True)\n",
    "\n",
    "\n",
    "class DataLoaderLite(Dataset):\n",
    "\n",
    "    tokenizer = None \n",
    "\n",
    "    def __init__(self, train_dataset_cocooptions, caption_length=20, tokenizer=tokenizer):\n",
    "        self.train_dataset_cocooptions = train_dataset_cocooptions\n",
    "        self.caption_length = caption_length\n",
    "        self.tokenizer = tokenizer \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_dataset_cocooptions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, image_captions = self.train_dataset_cocooptions[idx]\n",
    "\n",
    "         # Apply CLIPProcessor\n",
    "        image_tensor = clip_processor(images=img, return_tensors=\"pt\")\n",
    "        image_tensor = image_tensor[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        # prepend <START>, append <END>\n",
    "        caption = \"<START> \" + image_captions[0] + \" <END>\"\n",
    "\n",
    "        # tokenize with GPT2 tokenizer\n",
    "        tokens = self.tokenizer(\n",
    "            caption,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.caption_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return image_tensor, tokens[\"input_ids\"].squeeze(0), tokens[\"attention_mask\"].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "from torchvision.datasets import CocoCaptions, CocoDetection\n",
    "from torchvision import transforms\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "from pydantic import BaseModel \n",
    "import math \n",
    "from typing import ClassVar\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "from transformers import GPT2Config\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, input_ids, attention_mask = zip(*batch)\n",
    "    \n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    # pad input_ids and attention_mask to the max length in this batch\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return images, input_ids, attention_mask\n",
    "\n",
    "\n",
    "\n",
    "class ImageTextCollator:\n",
    "    def __init__(self, tokenizer, num_img_tokens=32, pad_token_id=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_img_tokens = num_img_tokens\n",
    "        self.pad_token_id = pad_token_id if pad_token_id is not None else tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        batch: list of tuples from Dataset: (image_tensor, input_ids, attention_mask)\n",
    "        returns dict with:\n",
    "            - images: stacked image tensors\n",
    "            - input_ids: padded text input ids\n",
    "            - attention_mask: padded + image prefix mask\n",
    "            - labels: text labels with image prefix masked\n",
    "        \"\"\"\n",
    "        imgs, input_ids_list, attention_masks_list = zip(*batch)\n",
    "        B = len(batch)\n",
    "\n",
    "        # Stack images\n",
    "        imgs = torch.stack(imgs, dim=0)  # (B, 3, H, W)\n",
    "\n",
    "        # Pad text input_ids and attention masks\n",
    "        max_len = max([x.size(0) for x in input_ids_list])\n",
    "        input_ids = torch.full((B, max_len), self.pad_token_id, dtype=torch.long)\n",
    "        attention_mask = torch.zeros((B, max_len), dtype=torch.long)\n",
    "\n",
    "        for i in range(B):\n",
    "            seq_len = input_ids_list[i].size(0)\n",
    "            input_ids[i, :seq_len] = input_ids_list[i]\n",
    "            attention_mask[i, :seq_len] = attention_masks_list[i]\n",
    "\n",
    "        # Image prefix mask\n",
    "        # img_mask = torch.ones((B, self.num_img_tokens), dtype=torch.long)\n",
    "        # attention_mask = torch.cat([img_mask, attention_mask], dim=1)\n",
    "\n",
    "        # Labels: pad image tokens\n",
    "        # labels = torch.cat([torch.full((B, self.num_img_tokens), self.pad_token_id, dtype=torch.long), input_ids], dim=1)\n",
    "\n",
    "        return {\n",
    "            \"images\": imgs,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "### --- DataSetup----####\n",
    "\n",
    "def setup_data(N, val_split=0.2):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #              std=[0.229, 0.224, 0.225])\n",
    "        # transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    train_dataset_cocooptions = CocoCaptions(\n",
    "        root='train2017',\n",
    "        annFile='annotations/captions_train2017.json',\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "\n",
    "    train_dataset_detection = CocoDetection(\n",
    "        root='train2017',\n",
    "        annFile='annotations/instances_train2017.json',\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    train_dataset_cocooptions = Subset(train_dataset_cocooptions, range(N))\n",
    "    train_dataset_detection = Subset(train_dataset_detection, range(N))\n",
    "\n",
    "    # split into train and validation\n",
    "    val_size = int(len(train_dataset_cocooptions) * val_split)\n",
    "    train_size = len(train_dataset_cocooptions) - val_size\n",
    "    train_dataset_cocooptions, val_dataset_cocooptions = random_split(train_dataset_cocooptions, [train_size, val_size])\n",
    "\n",
    "\n",
    "    val_size = int(len(train_dataset_detection) * val_split)\n",
    "    train_size = len(train_dataset_detection) - val_size\n",
    "    train_dataset_detection, val_dataset_detection = random_split(train_dataset_detection, [train_size, val_size])\n",
    "\n",
    "\n",
    "\n",
    "    return train_dataset_cocooptions, val_dataset_cocooptions, train_dataset_detection , val_dataset_detection\n",
    "\n",
    "\n",
    "class GPT2BlockWithCross(GPT2Block):\n",
    "    def __init__(self, config, embed_size):\n",
    "        super().__init__(config)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=embed_size, num_heads=12, batch_first=True)\n",
    "\n",
    "    def forward(self, hidden_states, img_feats=None, attention_mask=None, **kwargs):\n",
    "        # 1. Normal GPT2 block forward\n",
    "\n",
    "        out = super().forward(hidden_states, attention_mask=attention_mask, **kwargs)\n",
    "\n",
    "        # 2. Add cross attention\n",
    "        if img_feats is not None:\n",
    "            q = out[0]        # (B, T, D)\n",
    "            k = v = img_feats # (B, N, D)\n",
    "            img_mask = torch.zeros(img_feats.size(0), img_feats.size(1), dtype=torch.bool, device=img_feats.device)\n",
    "\n",
    "            cross, _ = self.cross_attn(q, k, v)\n",
    "            out = (out[0] + cross,) + out[1:]\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "#gpt_decoder = GPT2WithCross.from_pretrained(\"gpt2\")\n",
    "gpt_decoder = AutoModelForCausalLM.from_pretrained(decoder_model_name)\n",
    "gpt_decoder.config.pad_token_id = gpt_decoder.config.eos_token_id\n",
    "\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    gpt_hidden_size: ClassVar[int] = gpt_decoder.config.hidden_size\n",
    "    embed_size: ClassVar[int] = gpt_decoder.config.hidden_size\n",
    "    hidden_size: ClassVar[int] = gpt_decoder.config.hidden_size\n",
    "    input_channels: ClassVar[int] = 3  \n",
    "    image_h: ClassVar[int] = 224\n",
    "    image_w: ClassVar[int] = 224\n",
    "    steps: ClassVar[int] = 0\n",
    "    lr: ClassVar[float] = 1e-5\n",
    "    accumulation_steps: ClassVar[int] = 1\n",
    "    vocab_size: ClassVar[int] = gpt_decoder.get_input_embeddings().num_embeddings\n",
    "    number_of_items: ClassVar[int] = 400\n",
    "    batch_size:ClassVar[int] = 2\n",
    "    caption_len:ClassVar[int] = 20 \n",
    "    epochs: ClassVar[int] = 3\n",
    "\n",
    "\n",
    "#----- Model Setup -------\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "config = gpt_decoder.config\n",
    "gpt_decoder.resize_token_embeddings(gpt_decoder.get_input_embeddings().num_embeddings + 2)  # Example: add 3 tokens\n",
    "\n",
    "\n",
    "# patch every block\n",
    "# for i in range(len(gpt_decoder.transformer.h)):\n",
    "#     gpt_decoder.transformer.h[i] = GPT2BlockWithCross(config, ModelConfig.embed_size)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset_cocooptions, val_dataset_cocooptions, train_dataset_detection , val_dataset_detection = setup_data(ModelConfig.number_of_items)\n",
    "train_dataset_cocooptions = DataLoaderLite(train_dataset_cocooptions, caption_length=ModelConfig.caption_len, tokenizer=tokenizer)\n",
    "val_dataset_cocooptions = DataLoaderLite(val_dataset_cocooptions, caption_length=ModelConfig.caption_len, tokenizer=tokenizer)\n",
    "\n",
    "# collator = ImageTextCollator(tokenizer=tokenizer, num_img_tokens=32)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset_cocooptions, batch_size=ModelConfig.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset_cocooptions, batch_size=ModelConfig.batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "\n",
    "total_steps = len(train_dataloader)  * ModelConfig.epochs\n",
    "formatted_str = f\"Training details vocab size {ModelConfig.vocab_size} batch size {ModelConfig.batch_size} image size {ModelConfig.image_h}x{ModelConfig.image_w}\"\n",
    "formatted_str+= f\" total steps {total_steps} epochs {ModelConfig.epochs}\"\n",
    "formatted_str+= f\"Max loss {math.log(ModelConfig.vocab_size)}\"\n",
    "formatted_str+= f\"Perplexity {math.exp(math.log(ModelConfig.vocab_size))}\"\n",
    "\n",
    "print (formatted_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load Models \n",
    "import torch \n",
    "\n",
    "def freeze_model_layers(image_encoder, gpt_decoder):\n",
    "\n",
    "    # Freeze CLIP completely\n",
    "    for param in image_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "    # Unfreeze last 2 CLIP layers \n",
    "    # for layer in image_encoder.visual.transformer.resblocks[-2:]:\n",
    "    #     for param in layer.parameters():\n",
    "    #         param.requires_grad = True\n",
    "\n",
    "\n",
    "    # Freeze embeddings\n",
    "    # for p in gpt_decoder.transformer.word_embeddings.parameters():\n",
    "    #     p.requires_grad = False\n",
    "\n",
    "    for p in gpt_decoder.model.embed_tokens.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "\n",
    "    # Freeze lower 70% Falcon layers\n",
    "    # num_layers = len(gpt_decoder.transformer.h)\n",
    "    # freeze_until = int(0.7 * num_layers)\n",
    "    # for block in gpt_decoder.transformer.h[:freeze_until]:\n",
    "    #     for p in block.parameters():\n",
    "    #         p.requires_grad = False\n",
    "\n",
    "\n",
    "    num_layers = len(gpt_decoder.model.layers)\n",
    "    freeze_until = int(0.7 * num_layers)\n",
    "    for block in gpt_decoder.model.layers[:freeze_until]:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "\n",
    "    # Unfreeze top 30%\n",
    "    for block in gpt_decoder.model.layers[freeze_until:]:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "\n",
    "    # Trainable: last 30%\n",
    "    # for block in gpt_decoder.transformer.h[freeze_until:]:\n",
    "    #     for p in block.parameters():\n",
    "    #         p.requires_grad = True\n",
    "\n",
    "            \n",
    "\n",
    "    # Final LN + LM head\n",
    "    # for p in gpt_decoder.transformer.ln_f.parameters():\n",
    "    #     p.requires_grad = True\n",
    "    # for p in gpt_decoder.lm_head.parameters():\n",
    "    #     p.requires_grad = True\n",
    "\n",
    "\n",
    "    # Final norm + lm head\n",
    "    for p in gpt_decoder.model.norm.parameters():\n",
    "        p.requires_grad = True\n",
    "    for p in gpt_decoder.lm_head.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "\n",
    "    return image_encoder, gpt_decoder\n",
    "\n",
    "\n",
    "##### SETTING-UP MODEL ######\n",
    "\n",
    "# torch.mps.empty_cache(); import gc; gc.collect()\n",
    "# image_encoder = ResnetEncoder(ModelConfig.embed_size)\n",
    "image_encoder = CLIPEncoder(ModelConfig.embed_size)\n",
    "image_encoder = image_encoder.to(device)\n",
    "caption_encoder = ResnetGPT2Wrapper(gpt_decoder, ModelConfig.embed_size, ModelConfig.vocab_size)\n",
    "caption_encoder = caption_encoder.to(device)\n",
    "\n",
    "\n",
    "all_params = list([param for param in image_encoder.parameters() if param.requires_grad]) \n",
    "all_params +=  list([param for param in caption_encoder.parameters() if param.requires_grad]) \n",
    "\n",
    "print (f\" Before Trainable parameters in encoder model: {sum(p.numel() for p in all_params if p.requires_grad)/1e9} B\")\n",
    "\n",
    "image_encoder, gpt_decoder = freeze_model_layers(image_encoder, gpt_decoder)\n",
    "\n",
    "\n",
    "# image_encoder.gradient_checkpointing_enable()\n",
    "gpt_decoder.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "all_params = list([param for param in image_encoder.parameters() if param.requires_grad]) \n",
    "all_params +=  list([param for param in caption_encoder.parameters() if param.requires_grad]) \n",
    "\n",
    "print (f\"After Trainable parameters in encoder model: {sum(p.numel() for p in all_params if p.requires_grad)/1e9} B\")\n",
    "torch.mps.empty_cache()\n",
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5586223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "\n",
    "def eval():\n",
    "    # --------------------\n",
    "    #  Validation step\n",
    "    # --------------------\n",
    "    caption_encoder.eval()\n",
    "    image_encoder.eval()\n",
    "    val_loss = 0\n",
    "    count = 0 \n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_dataloader:\n",
    "            image_tensor, caption_tensor, attention_mask = [x.to(device) for x in val_batch]\n",
    "            with torch.autocast(\"mps\", enabled=True, dtype=torch.bfloat16):\n",
    "                x_embed = image_encoder(image_tensor)\n",
    "                _, val_caption_loss = caption_encoder(x_embed, caption_tensor, attention_mask)\n",
    "            val_loss += val_caption_loss.item()\n",
    "            count+=1\n",
    "            if count > 2:break \n",
    "    val_loss /= count \n",
    "    caption_encoder.train()\n",
    "    image_encoder.train()\n",
    "    print(f\"Epoch {epoch+1}: train_loss={total_loss/len(train_dataloader):.4f}, val_loss={val_loss:.4f}\")\n",
    "    return val_loss \n",
    "\n",
    "\n",
    "\n",
    "def should_stop(loss_list):\n",
    "    last_ten_loss = loss_list[-20:]\n",
    "    threshold = 0.4\n",
    "    if len(last_ten_loss)==20 and len(loss_list)>=20:\n",
    "        diffs = np.diff(last_ten_loss)\n",
    "        step_trends = []\n",
    "        for d in diffs:\n",
    "            if d > threshold:\n",
    "                step_trends.append(\"increasing\")\n",
    "            elif d < -threshold:\n",
    "                step_trends.append(\"decreasing\")\n",
    "            else:\n",
    "                step_trends.append(\"steady\")\n",
    "\n",
    "        if all(t == \"steady\" for t in step_trends):\n",
    "            return True \n",
    "        else:\n",
    "            print (\"Trend\", step_trends)\n",
    "    return False \n",
    "\n",
    "\n",
    "\n",
    "##### Setup Training #####\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "all_params = list([param for param in image_encoder.parameters() if param.requires_grad]) \n",
    "all_params +=  list([param for param in caption_encoder.parameters() if param.requires_grad]) \n",
    "\n",
    "print (f\"Trainable parameters in encoder model: {sum(p.numel() for p in all_params if p.requires_grad)/1e6} M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(all_params, lr=ModelConfig.lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps/ModelConfig.accumulation_steps, eta_min=1e-6)\n",
    "import time \n",
    "start_time = time.time()\n",
    "total_loss = 0 \n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "steps_no_improve = 0\n",
    "patience_steps = 10\n",
    "stop = False \n",
    "\n",
    "\n",
    "for epoch in range(ModelConfig.epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2] # [B, 3, 224, 224], [B, T], [B, T] \n",
    "        image_tensor, caption_tensor, attention_mask = image_tensor.to(device), caption_tensor.to(device), attention_mask.to(device)\n",
    "        B, C, H, W = image_tensor.shape\n",
    "\n",
    "        global_step = epoch * len(train_dataloader) + step + 1     \n",
    "\n",
    "        with torch.autocast(\"mps\", enabled=True, dtype=torch.bfloat16):\n",
    "            x_embed = image_encoder(image_tensor) # (B, N, embed_size) \n",
    "            logits, caption_loss  = caption_encoder(x_embed, caption_tensor, attention_mask)  # (B, T-1, vocab_size)\n",
    "            loss = caption_loss / ModelConfig.accumulation_steps  \n",
    "\n",
    "        # x_embed = image_encoder(image_tensor) # (B, N, embed_size) \n",
    "        # logits, caption_loss  = caption_encoder(x_embed, caption_tensor, attention_mask)  # (B, T-1, vocab_size)\n",
    "        # loss = caption_loss / ModelConfig.accumulation_steps  \n",
    "\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        all_params = list([param for param in image_encoder.parameters() if param.requires_grad]) + list(caption_encoder.parameters())\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, max_norm=5.0)\n",
    "\n",
    "        if (step + 1) % ModelConfig.accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * ModelConfig.accumulation_steps  \n",
    "\n",
    "\n",
    "        if global_step %100==0:\n",
    "            val_loss = eval()\n",
    "            loss_list.append(val_loss)\n",
    "            if should_stop(loss_list):\n",
    "                stop = True \n",
    "                break\n",
    "\n",
    "          # estimate remaining time every 100 steps\n",
    "        if global_step % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            steps_per_sec = global_step / elapsed\n",
    "            remaining_steps = total_steps - global_step\n",
    "            est_remaining = remaining_steps / steps_per_sec\n",
    "            est_total = total_steps / steps_per_sec\n",
    "\n",
    "            print(f\"epoch {epoch+1}/{ModelConfig.epochs} step {step}/{len(train_dataloader)} \"\n",
    "                  f\"Loss: {loss.item()*ModelConfig.accumulation_steps:.4f} | \"\n",
    "                  f\"Elapsed: {elapsed/60:.2f} min | \"\n",
    "                  f\"ETA: {est_remaining/60:.2f} min | \"\n",
    "                  f\"Total est: {est_total/60:.2f} min | \"\n",
    "                  f\"Memory: {torch.mps.current_allocated_memory() / 1e9:.2f} GB , \\ {torch.mps.driver_allocated_memory() / 1e9:.2f} GB | \"\n",
    "                  f\"Perplexity {math.exp(loss.item()*ModelConfig.accumulation_steps):.2f}\"\n",
    "                  )\n",
    "            \n",
    "            # save_model(image_encoder=image_encoder, caption_encoder=caption_encoder)\n",
    "\n",
    "    if (step + 1) % ModelConfig.accumulation_steps != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, 5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "    if stop:\n",
    "        break\n",
    "    \n",
    "    del image_tensor, caption_tensor, x_embed, logits\n",
    "    torch.mps.empty_cache()\n",
    "    import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed806315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "device=\"mps\" \n",
    "\n",
    "def generate_caption(image_tensor, max_len=50, use_image=True):\n",
    "    caption_encoder.eval()\n",
    "    image_encoder.eval()\n",
    "\n",
    "    image_tensor = image_tensor.to(device).unsqueeze(0)  # (1, 3, 224, 224)\n",
    "\n",
    "    # Encode image\n",
    "    if use_image:\n",
    "        x_embed = image_encoder(image_tensor)  # (1, N, D)\n",
    "    else:\n",
    "        x_embed = torch.zeros((1, 196, ModelConfig.embed_size), device=device)\n",
    "\n",
    "    # Start token\n",
    "    start_id = tokenizer.convert_tokens_to_ids(\"<START>\")\n",
    "    input_ids = torch.tensor([[start_id]], device=device)\n",
    "    generated_ids = input_ids\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            # attn_mask = torch.cat([torch.ones(1, caption_encoder.num_img_tokens, device=device),\n",
    "            #     torch.ones(1, generated_ids.shape[1], device=device)\n",
    "            # ], dim=1)\n",
    "\n",
    "            attn_mask = torch.ones(1, generated_ids.shape[1], device=device)\n",
    "           \n",
    "\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, _ = caption_encoder(x_embed, generated_ids, attn_mask, mode=\"test\")\n",
    "\n",
    "            next_token_logits = logits[:, -1, :]  # (1, vocab_size)\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Append\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "\n",
    "        if next_token_id.item() == tokenizer.convert_tokens_to_ids(\"<END>\"):\n",
    "            break\n",
    "\n",
    "    # Decode\n",
    "    caption = tokenizer.decode(generated_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "\n",
    "counter = 0 \n",
    "# Example usage\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2] # [B, 3, 224, 224], [B, T], [B, T] \n",
    "    image_tensor, caption_tensor, attention_mask = image_tensor.to(device), caption_tensor.to(device), attention_mask.to(device)\n",
    "    B, C, H, W = image_tensor.shape\n",
    "\n",
    "    #caption_without_image = generate_caption(encoder_model, caption_encoder, image_tensor[0], tokenizer, use_image=False)\n",
    "\n",
    "    caption_with_image = generate_caption(image_tensor[0], use_image=True)\n",
    "\n",
    "    plt.imshow(image_tensor[0].permute(1,2,0).cpu().numpy())\n",
    "    print(\"With image context: \", caption_with_image)\n",
    "    print (\"Actual\", tokenizer.decode(caption_tensor[0].tolist(), skip_special_tokens=True))\n",
    "    # print(\"Without image context: \", caption_without_image)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodel (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
