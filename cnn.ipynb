{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0bc8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist \n",
    "import matplotlib.pyplot as plot \n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "\n",
    "digits = train_images[:10]\n",
    "labels = train_labels[:10]\n",
    "\n",
    "\n",
    "print(digits.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = digits[0,:,:]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f0400",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e2567",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.imshow(x, vmin=0, vmax=255, cmap=\"grey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "from tensorflow.keras.datasets import mnist \n",
    "import matplotlib.pyplot as plot \n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "\n",
    "digits = train_images[:10]\n",
    "labels = train_labels[:10]\n",
    "\n",
    "\n",
    "print(digits.shape) \n",
    "\n",
    "# Define the Laplacian filter\n",
    "laplacian_filter = np.array([\n",
    "    [-1, -1, -1],\n",
    "    [-1,  8, -1],\n",
    "    [-1, -1, -1]\n",
    "])\n",
    "\n",
    "digit = digits[0]  \n",
    "H, W = digit.shape[0], digit.shape[1]\n",
    "digit_padded  = np.zeros((H+2, W+2))\n",
    "\n",
    "digit_padded[1:H+1, 1:W+1] = digit \n",
    "\n",
    "\n",
    "\n",
    "filter_results = np.zeros((digit.shape[0], digit.shape[1]))\n",
    "\n",
    "for i in range(digit.shape[0]):\n",
    "    for j in range(digit.shape[1]):\n",
    "        region = digit_padded[i:i+3, j:j+3]\n",
    "        elem_wise_mat = region * laplacian_filter\n",
    "        filter_results[i, j] = np.sum(elem_wise_mat)\n",
    "\n",
    "\n",
    "#strideloop\n",
    "\n",
    "stride=2 \n",
    "\n",
    "H =  (digit.shape[0] + 2 -3)//3 + 1\n",
    "W =  (digit.shape[1] + 2 -3)//3 + 1\n",
    "\n",
    "\n",
    "filter_results_1 = np.zeros((H, W))\n",
    "for i in range(0, H + 2 - 3 + 1, stride):\n",
    "    for j in range(0, W + 2 - 3 + 1, stride):\n",
    "        region = digit_padded[i:i+3, j:j+3]\n",
    "        elem_wise_mat = region * laplacian_filter\n",
    "        filter_results_1[i//stride, j//stride] = np.sum(elem_wise_mat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31490c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57237b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_results = np.zeros((digit.shape[0], digit.shape[1]))\n",
    "\n",
    "for i in range(digit.shape[0]):\n",
    "    for j in range(digit.shape[1]):\n",
    "        region = digit_padded[i:i+3, j:j+3]\n",
    "        elem_wise_mat = region * laplacian_filter\n",
    "        filter_results[i, j] = np.sum(elem_wise_mat)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe96c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.imshow(filter_results, cmap=\"grey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17febea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.imshow(filter_results_1, cmap=\"grey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd84c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.datasets import  mnist \n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "def conv2d(x, conv_filter, stride, padding):\n",
    "    H, W = x.shape \n",
    "\n",
    "    x_padded = torch.zeros(H+2*padding, W+2*padding, device=device)\n",
    "    x_padded[padding:H+padding, padding:W+padding] = x \n",
    "\n",
    "    kH, kW = conv_filter.shape\n",
    "    \n",
    "    out_H = (H + 2*padding - kH)//stride + 1 \n",
    "    out_W = (W + 2*padding - kW)//stride + 1 \n",
    "\n",
    "    output_map = torch.zeros(out_H, out_W, device=device)\n",
    "\n",
    "    for i in range(0, out_H*stride, stride):\n",
    "        for j in range(0, out_W*stride, stride):\n",
    "            output_map[i//stride, j//stride] = torch.sum((x_padded[i:i+kH, j:j+kW] * conv_filter))\n",
    "    return output_map\n",
    "\n",
    "\n",
    "class ConvolutionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, number_of_filter, padding, stride, filter_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.filters = nn.Parameter(torch.randn(number_of_filter, filter_size, filter_size) * 0.1)\n",
    "        self.number_of_filter = number_of_filter\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.filter_size = filter_size \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  \n",
    "\n",
    "        output = [] \n",
    "        \n",
    "        for b in range(B):\n",
    "            feature_map = []\n",
    "            for j in range(self.number_of_filter):\n",
    "                output_conv2d = conv2d(x[b, 0], self.filters[j].to(device),  self.stride, self.padding)\n",
    "                feature_map.append(output_conv2d)\n",
    "            output.append(torch.stack(feature_map))\n",
    "        return torch.stack(output)\n",
    "\n",
    "\n",
    "class CNNMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = ConvolutionLayer(number_of_filter=2, padding=1, stride=1, filter_size=3)\n",
    "        self.conv2 = ConvolutionLayer(number_of_filter=2, padding=1, stride=1, filter_size=3)\n",
    "        num_features = 2*28*28\n",
    "        self.classifier = nn.Linear(num_features, classes, device=device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten [B, 8*28*28]\n",
    "        x = self.classifier(x) \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 2\n",
    "epochs = 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.datasets import  mnist \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "def conv2d_vectorized(x, conv_filter, stride=1, padding=0):\n",
    "    # x: [H, W], conv_filter: [kH, kW]\n",
    "    x = x.unsqueeze(0).unsqueeze(0)  # [1,1,H,W]\n",
    "    x_unf = F.unfold(x, kernel_size=conv_filter.shape, stride=stride, padding=padding)\n",
    "    # x_unf: [1, kH*kW, out_H*out_W]\n",
    "    conv_flat = conv_filter.flatten().unsqueeze(1)  # [kH*kW, 1]\n",
    "    out = torch.matmul(conv_flat.T, x_unf)  # [1, out_H*out_W]\n",
    "    out_H = (x.shape[2] + 2*padding - conv_filter.shape[0]) // stride + 1\n",
    "    out_W = (x.shape[3] + 2*padding - conv_filter.shape[1]) // stride + 1\n",
    "    return out.view(1, out_H, out_W).squeeze(0)\n",
    "\n",
    "\n",
    "def conv2d(x, conv_filter, stride, padding):\n",
    "    H, W = x.shape \n",
    "\n",
    "    x_padded = torch.zeros(H+2*padding, W+2*padding, device=device)\n",
    "    x_padded[padding:H+padding, padding:W+padding] = x \n",
    "\n",
    "    kH, kW = conv_filter.shape\n",
    "    \n",
    "    out_H = (H + 2*padding - kH)//stride + 1 \n",
    "    out_W = (W + 2*padding - kW)//stride + 1 \n",
    "\n",
    "    output_map = torch.zeros(out_H, out_W, device=device)\n",
    "\n",
    "    for i in range(0, out_H*stride, stride):\n",
    "        for j in range(0, out_W*stride, stride):\n",
    "            output_map[i//stride, j//stride] = torch.sum((x_padded[i:i+kH, j:j+kW] * conv_filter))\n",
    "    return output_map\n",
    "\n",
    "\n",
    "class ConvolutionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, padding, stride, filter_size):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.filters = nn.Parameter(torch.randn(number_of_filter, filter_size, filter_size) * 0.1)\n",
    "        self.filters = nn.Parameter(torch.randn(output_channels, input_channels, filter_size, filter_size, device=device) * 0.1)\n",
    "\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.filter_size = filter_size \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  ### [B, C, H, W]\n",
    "\n",
    "        filters_flat = self.filters.view(self.output_channels, -1) \n",
    "\n",
    "        x_unf = F.unfold(x, kernel_size=self.filter_size, padding=self.padding, stride=self.stride)\n",
    "\n",
    "        filters_flat_exp = filters_flat.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "\n",
    "        out = torch.bmm(filters_flat_exp, x_unf)\n",
    "        out_H = (H + 2*self.padding - self.filter_size)//self.stride + 1\n",
    "        out_W = (W + 2*self.padding - self.filter_size)//self.stride + 1\n",
    "        out = out.view(B, self.number_of_filter, out_H, out_W)\n",
    "        return out\n",
    "\n",
    "\n",
    "        # output = [] \n",
    "        \n",
    "        # for b in range(B):\n",
    "        #     feature_map = []\n",
    "        #     for j in range(self.number_of_filter):\n",
    "        #         #output_conv2d = conv2d(x[b, 0], self.filters[j].to(device),  self.stride, self.padding)\n",
    "        #         output_conv2d = conv2d_vectorized(x[b, 0], self.filters[j].to(device),  self.stride, self.padding)\n",
    "        #         feature_map.append(output_conv2d) ##[1, out_H, out_W]\n",
    "        #     output.append(torch.stack(feature_map)) ##[F, out_H, out_W]\n",
    "        # return torch.stack(output) ##[B, F, out_H, out_W]\n",
    "    \n",
    "\n",
    "class CNNMOdel(nn.Module):\n",
    "\n",
    "    def __init__(self, classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = ConvolutionLayer(input_channels=1, output_channels=2 padding=1, stride=1, filter_size=3)\n",
    "        self.conv2 = ConvolutionLayer(input_channels=2, output_channels=2, padding=1, stride=1, filter_size=3)\n",
    "        num_features = 2*28*28\n",
    "        self.classifier = nn.Linear(num_features, classes, device=device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten [B, 8*28*28]\n",
    "        x = self.classifier(x) \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images[:10]\n",
    "train_labels = train_labels[:10]\n",
    "test_images = test_images[:10]\n",
    "test_labels = test_labels[:10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert to float tensors and normalize\n",
    "train_images = torch.tensor(train_images, dtype=torch.float32) / 255.0\n",
    "test_images = torch.tensor(test_images, dtype=torch.float32) / 255.0\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Add channel dimension: (N, C, H, W)\n",
    "train_images = train_images.unsqueeze(1)  # (N, 1, 28, 28)\n",
    "test_images = test_images.unsqueeze(1)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = torch.utils.data.TensorDataset(train_images, train_labels)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Model\n",
    "model = CNNMOdel(classes=10)\n",
    "model  = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "total_steps = epochs * train_loader.__len__()\n",
    "\n",
    "steps = 0 \n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        steps+=1\n",
    "\n",
    "        if steps%5==0:\n",
    "            print (f\"steps {steps} Loss {loss.item()}\")\n",
    "            \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ed8a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=3.13s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from torchvision.datasets import CocoCaptions, CocoDetection\n",
    "from torchvision import transforms\n",
    "import torch \n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset_cocooptions = CocoCaptions(\n",
    "    root='train2017',\n",
    "    annFile='annotations/captions_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset_detection = CocoDetection(\n",
    "    root='train2017',\n",
    "    annFile='annotations/instances_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "N = 1000\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "train_dataset_cocooptions = Subset(train_dataset_cocooptions, range(N))\n",
    "train_dataset_detection = Subset(train_dataset_detection, range(N))\n",
    "\n",
    "\n",
    "all_captions = \"\\n\".join([caption for captions_list in train_dataset_cocooptions for caption in captions_list[1]])\n",
    "all_words = list(all_captions.split(\" \"))\n",
    "\n",
    "\n",
    "\n",
    "counter = Counter()\n",
    "for word in all_words:\n",
    "    counter[word]+=1\n",
    "\n",
    "vocab = [word for word, cnt in counter.items() if cnt>5]\n",
    "vocab +=[\"UNK\", \"<START>\", \"<END>\", \"<PAD>\"]\n",
    "\n",
    "\n",
    "word2idx =  {item:i for i, item in enumerate(vocab)}\n",
    "idx2word =  {i:item for i, item in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def encode(stri):\n",
    "    all_tensor = [word2idx.get(word, word2idx[\"UNK\"]) for word in stri.split(\" \")]\n",
    "    return all_tensor \n",
    "\n",
    "def decode(input_tensor):\n",
    "    return [idx2word[each] for each in input_tensor]\n",
    "    \n",
    "\n",
    "class DataLoaderLite(Dataset):\n",
    "\n",
    "    def __init__(self, train_dataset_cocooptions, caption_length=50):\n",
    "        self.train_dataset_cocooptions = train_dataset_cocooptions\n",
    "        self.caption_length = caption_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_dataset_cocooptions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_tensor, image_captions = self.train_dataset_cocooptions[idx]\n",
    "        caption = \"<START> \" + image_captions[0] + \" <END>\"\n",
    "        caption_tensor = encode(caption)\n",
    "\n",
    "        if len(caption_tensor) < self.caption_length:\n",
    "            caption_tensor += [word2idx[\"<PAD>\"]] * (self.caption_length - len(caption_tensor))\n",
    "\n",
    "        else:\n",
    "            caption_tensor = caption_tensor[:self.caption_length]\n",
    "        return image_tensor, torch.tensor(caption_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f69fe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:560: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:560: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/var/folders/ww/qr_kh7fj37j544l07ttgdbfm0000gn/T/ipykernel_3108/166552445.py:560: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  f\"Memory: {torch.mps.current_allocated_memory() / 1e9:.2f} GB , \\ {torch.mps.driver_allocated_memory() / 1e9:.2f} GB | \"\n",
      "/Users/preetamverma/Desktop/multimodel/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.23s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=3.07s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "/var/folders/ww/qr_kh7fj37j544l07ttgdbfm0000gn/T/ipykernel_3108/166552445.py:470: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/Users/preetamverma/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training details vocab size 50259 batch size 2 image size 224x224 total steps 500 epochs 1Max loss 10.824944914361643Perplexity 50259.00000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/preetamverma/Desktop/multimodel/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/preetamverma/Desktop/multimodel/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading caption model\n",
      "Trainable parameters in encoder model:\n",
      "153437184\n",
      "Starting epoch 1/1\n",
      "Step 1/500 Global step 1/500\n",
      "Running encoder model\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "from torch.functional import F \n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from torchvision.datasets import CocoCaptions, CocoDetection\n",
    "from torchvision import transforms\n",
    "import torch \n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "# add special tokens\n",
    "special_tokens = {\"additional_special_tokens\": [\"<START>\", \"<END>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# store the <PAD> token too (GPT2 doesn’t have one by default)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(\"<PAD>\")\n",
    "start_token_id = tokenizer.convert_tokens_to_ids(\"<START>\") \n",
    "end_token_id = tokenizer.convert_tokens_to_ids(\"<END>\") \n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset_cocooptions = CocoCaptions(\n",
    "    root='train2017',\n",
    "    annFile='annotations/captions_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset_detection = CocoDetection(\n",
    "    root='train2017',\n",
    "    annFile='annotations/instances_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "N = 1000\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "train_dataset_cocooptions = Subset(train_dataset_cocooptions, range(N))\n",
    "train_dataset_detection = Subset(train_dataset_detection, range(N))\n",
    "\n",
    "\n",
    "# all_captions = \"\\n\".join([caption for captions_list in train_dataset_cocooptions for caption in captions_list[1]])\n",
    "# all_words = list(all_captions.split(\" \"))\n",
    "\n",
    "\n",
    "\n",
    "# counter = Counter()\n",
    "# for word in all_words:\n",
    "#     counter[word]+=1\n",
    "\n",
    "# vocab = [word for word, cnt in counter.items() if cnt>5]\n",
    "# vocab +=[\"UNK\", \"<START>\", \"<END>\", \"<PAD>\"]\n",
    "\n",
    "\n",
    "# word2idx =  {item:i for i, item in enumerate(vocab)}\n",
    "# idx2word =  {i:item for i, item in enumerate(vocab)}\n",
    "\n",
    "\n",
    "# def encode(stri):\n",
    "#     all_tensor = [word2idx.get(word, word2idx[\"UNK\"]) for word in stri.split(\" \")]\n",
    "#     return all_tensor \n",
    "\n",
    "# def decode(input_tensor):\n",
    "#     return [idx2word[each] for each in input_tensor]\n",
    "\n",
    "\n",
    "class DataLoaderLite(Dataset):\n",
    "    def __init__(self, train_dataset_cocooptions, caption_length=50, tokenizer=tokenizer):\n",
    "        self.train_dataset_cocooptions = train_dataset_cocooptions\n",
    "        self.caption_length = caption_length\n",
    "        self.tokenizer = tokenizer \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_dataset_cocooptions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_tensor, image_captions = self.train_dataset_cocooptions[idx]\n",
    "\n",
    "        # prepend <START>, append <END>\n",
    "        caption = \"<START> \" + image_captions[0] + \" <END>\"\n",
    "\n",
    "        # tokenize with GPT2 tokenizer\n",
    "        tokens = self.tokenizer(\n",
    "            caption,\n",
    "            max_length=self.caption_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return image_tensor, tokens[\"input_ids\"].squeeze(0), tokens[\"attention_mask\"].squeeze(0)\n",
    "    \n",
    "    # def __getitem__(self, idx):\n",
    "    #     image_tensor, image_captions = self.train_dataset_cocooptions[idx]\n",
    "\n",
    "    #     # Prepend <IMG> + <START>, Append <END>\n",
    "    #     caption = \"<IMG> <START> \" + image_captions[0] + \" <END>\"\n",
    "    #     caption_tensor = encode(caption)\n",
    "\n",
    "    #     if len(caption_tensor) < self.caption_length:\n",
    "    #         caption_tensor += [word2idx[\"<PAD>\"]] * (self.caption_length - len(caption_tensor))\n",
    "    #     else:\n",
    "    #         caption_tensor = caption_tensor[:self.caption_length]\n",
    "\n",
    "    #     return image_tensor, torch.tensor(caption_tensor)\n",
    "\n",
    "    \n",
    "\n",
    "# class DataLoaderLite(Dataset):\n",
    "\n",
    "#     def __init__(self, train_dataset_cocooptions, caption_length=50):\n",
    "#         self.train_dataset_cocooptions = train_dataset_cocooptions\n",
    "#         self.caption_length = caption_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.train_dataset_cocooptions)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         image_tensor, image_captions = self.train_dataset_cocooptions[idx]\n",
    "#         caption = \"<START> \" + image_captions[0] + \" <END>\"\n",
    "#         caption_tensor = encode(caption)\n",
    "\n",
    "#         if len(caption_tensor) < self.caption_length:\n",
    "#             caption_tensor += [word2idx[\"<PAD>\"]] * (self.caption_length - len(caption_tensor))\n",
    "\n",
    "#         else:\n",
    "#             caption_tensor = caption_tensor[:self.caption_length]\n",
    "#         return image_tensor, torch.tensor(caption_tensor)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "def conv2d(x, kernel, stride, padding):\n",
    "    H, W = x.shape\n",
    "    device = x.device \n",
    "    x_padded = torch.zeros((H+2*padding, W+2*padding), device=device)\n",
    "    x_padded[padding:H+padding, padding:W+padding] = x \n",
    "\n",
    "    kH, kW = kernel.shape \n",
    "    out_H = (H+2*padding-kH)//stride +1\n",
    "    out_W = (W+2*padding-kW)//stride +1\n",
    "\n",
    "    feature_map = torch.zeros((out_H, out_W), device=device)\n",
    "\n",
    "    for i in range(0, (H+2*padding-kH+1), stride):\n",
    "        for j in range(0, (W+2*padding-kW+1), stride):\n",
    "            region = x_padded[i:i+kH, j:j+kW] \n",
    "            feature_map[i//stride, j//stride] = torch.sum(kernel.to(region.device) * region)\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "\n",
    "class ConvolutionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, padding, stride, kernel_size):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.padding = padding\n",
    "        self.stride = stride \n",
    "        self.kernel_size = kernel_size \n",
    "        self.kernel = nn.Parameter(torch.randn(self.output_channels, self.input_channels, self.kernel_size, self.kernel_size))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        output = []\n",
    "        for batch in range(B):\n",
    "            output_feature_map = []\n",
    "            for each_output_channel in range(self.output_channels):\n",
    "                feature_map = torch.zeros(((H+2*self.padding-self.kernel_size)//self.stride +1, (W+2*self.padding-self.kernel_size)//self.stride +1), device=device)\n",
    "\n",
    "                for each_input_channel in range(self.input_channels):\n",
    "                    feature_map += conv2d(x[batch, each_input_channel], self.kernel[each_output_channel, each_input_channel], self.stride, self.padding)\n",
    "                output_feature_map.append(feature_map)\n",
    "            output.append(torch.stack(output_feature_map))\n",
    "        return torch.stack(output)\n",
    "    \n",
    "\n",
    "\n",
    "class ResnetGPT2Wrapper(nn.Module):\n",
    "    def __init__(self, gpt_decoder, embed_size, vocab_size, num_img_tokens=5):\n",
    "        super().__init__()\n",
    "        self.gpt_decoder = gpt_decoder\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_img_tokens = num_img_tokens\n",
    "\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_size, num_heads=4, batch_first=True)\n",
    "        self.key_proj = nn.Linear(embed_size, embed_size, dtype=torch.float32)\n",
    "        self.value_proj = nn.Linear(embed_size, embed_size, dtype=torch.float32)\n",
    "        self.query_proj = nn.Linear(embed_size, embed_size, dtype=torch.float32)\n",
    "        self.layernorm = nn.LayerNorm(embed_size, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.img_queries = nn.Parameter(torch.randn(num_img_tokens, embed_size) * 0.01)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, img_features, captions_tensor, attention_mask=None):\n",
    "\n",
    "\n",
    "        print (\"Image features shape:\", img_features.shape)\n",
    "\n",
    "        img_features = img_features.float()\n",
    "         # 1. Token embeddings from GPT2\n",
    "        tok_embeds = self.gpt_decoder.transformer.wte(captions_tensor)  # (B, T, D)\n",
    "\n",
    "        B = tok_embeds.shape[0]\n",
    "\n",
    "        print (\"Batch size \", B)\n",
    "\n",
    "        \n",
    "\n",
    "        queries = self.img_queries.unsqueeze(0).expand(B, -1, -1)  # (B, num_img_tokens, D)\n",
    "\n",
    "\n",
    "        B, T, D = tok_embeds.shape\n",
    "        N = img_features.shape[1]\n",
    "\n",
    "       \n",
    "\n",
    "        k = self.key_proj(img_features)              # (B, N, D)\n",
    "        v = self.value_proj(img_features)            # (B, N, D)\n",
    "        \n",
    "        enriched, _ = self.mha(self.query_proj(queries), k, v)  # (B, M, D)\n",
    "\n",
    "        enriched = self.layernorm(queries + enriched) \n",
    "\n",
    "\n",
    "\n",
    "        fused = torch.cat([enriched, tok_embeds], dim=1)  # (B, M+T, D)\n",
    "\n",
    "        # query = self.query_proj(tok_embeds)\n",
    "        # keys  = self.key_proj(img_features)\n",
    "        # values = self.value_proj(img_features)\n",
    "        # enriched, attn_weights = self.mha(query, keys, values)\n",
    "\n",
    "        enriched = self.dropout(fused)\n",
    "\n",
    "\n",
    "        # enriched = enriched + tok_embeds  # residual connection\n",
    "\n",
    "       # 4. Shift inputs for teacher forcing: predict next token\n",
    "        inputs_embeds = enriched[:, :-1, :].contiguous()\n",
    "        labels = captions_tensor[:, 1:].contiguous()\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            img_mask = torch.ones(B, self.num_img_tokens, device=attention_mask.device)\n",
    "            attention_mask = torch.cat([img_mask, attention_mask], dim=1)\n",
    "            attention_mask = attention_mask[:, :-1].contiguous()\n",
    "\n",
    "        # 5. Pass enriched embeddings into GPT2\n",
    "        outputs = self.gpt_decoder(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        # return logits directly (B, T-1, V)\n",
    "        return outputs.logits\n",
    "\n",
    "    \n",
    "class LSTMEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm_layer = nn.LSTM(2*embed_size, hidden_size=hidden_size, batch_first=True, num_layers=3)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_size, num_heads=4, batch_first=True)\n",
    "        self.key_proj = nn.Linear(embed_size, embed_size, dtype=torch.float32)\n",
    "        self.value_proj = nn.Linear(embed_size, embed_size, dtype=torch.float32)\n",
    "        self.query_proj = nn.Linear(embed_size, embed_size, dtype=torch.float32)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embedding_layer(captions[:, :-1])  # teacher forcing\n",
    "        # features = features.unsqueeze(1)  # (B, 1, embed_size)\n",
    "\n",
    "        \n",
    "        query = self.query_proj(embeddings)\n",
    "\n",
    "        keys  = self.key_proj(features) \n",
    "\n",
    "        values = self.value_proj(features)\n",
    "\n",
    "        # keys = keys.unsqueeze(1)\n",
    "        # values = values.unsqueeze(1)\n",
    "\n",
    "\n",
    "        attn_out, attn_weights = self.mha(query, keys, values)\n",
    "\n",
    "\n",
    "        attn_out = torch.cat((embeddings, attn_out), dim=-1)\n",
    "\n",
    "\n",
    "        # print (f\"==== attn_weights\", attn_weights.shape)\n",
    "        # LLLL\n",
    "\n",
    "        # inputs = torch.cat((attn_out, embeddings), dim=1)\n",
    "        outputs, _ = self.lstm_layer(attn_out)\n",
    "\n",
    "\n",
    "        outputs = self.fc(outputs)  # (B, T, vocab_size)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_h, grid_w):\n",
    "    \"\"\"Return 2D sine-cosine positional embeddings\"\"\"\n",
    "    grid_y = torch.arange(grid_h, dtype=torch.float32)\n",
    "    grid_x = torch.arange(grid_w, dtype=torch.float32)\n",
    "    grid = torch.meshgrid(grid_y, grid_x, indexing='ij')  # (H, W)\n",
    "    grid = torch.stack(grid, dim=-1)  # (H, W, 2)\n",
    "\n",
    "    # flatten\n",
    "    grid = grid.reshape(-1, 2)  # (H*W, 2)\n",
    "\n",
    "    # compute embeddings\n",
    "    pos_emb = []\n",
    "    for dim in range(embed_dim // 2):\n",
    "        div_term = 10000 ** (2 * (dim // 2) / embed_dim)\n",
    "        pos_emb.append(torch.sin(grid / div_term) if dim % 2 == 0 else torch.cos(grid / div_term))\n",
    "    pos_emb = torch.cat(pos_emb, dim=1)  # (H*W, embed_dim)\n",
    "    return pos_emb\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "\n",
    "class ResnetEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, freeze_until_layer=5):\n",
    "        super().__init__()\n",
    "        # load pretrained ResNet\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]  # remove the last fc layer\n",
    "        self.backbone = nn.Sequential(*modules)\n",
    "\n",
    "        # freeze layers\n",
    "        child_counter = 0\n",
    "        for child in self.backbone.children():\n",
    "            if child_counter < freeze_until_layer:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            child_counter += 1\n",
    "       \n",
    "\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, 3, 224, 224) -> (B, 2048, H/32, W/32)\n",
    "        feats = self.backbone(x)\n",
    "        B, C, H, W = feats.shape\n",
    "        feats = feats.view(B, C, -1).permute(0, 2, 1)  # (B, H*W, C)\n",
    "        feats = self.fc(feats)  # (B, H*W, embed_size)\n",
    "        return feats\n",
    "\n",
    "\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, input_shape):\n",
    "        super().__init__()\n",
    "   \n",
    "        # More filters + strides to reduce spatial dims\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)  # 224 -> 112\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1) # 112 -> 56\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1) # 56 -> 28\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1) # 28 -> 14\n",
    "\n",
    "        pos_emb = get_2d_sincos_pos_embed(embed_size, 14, 14)  # (196, embed_size)\n",
    "        self.register_buffer(\"pos_embed\", pos_emb.unsqueeze(0))  # (1, 196, embed_size)\n",
    "\n",
    "\n",
    "        self.to(device)\n",
    "        with torch.no_grad():\n",
    "            B, C, H, W = input_shape[:]\n",
    "            x_dummy = torch.randn((B, C, H, W), device=device)\n",
    "            x_dummy = self.conv1(x_dummy)\n",
    "            x_dummy = self.conv2(x_dummy)\n",
    "            x_dummy = self.conv3(x_dummy)\n",
    "            x_dummy = self.conv4(x_dummy)\n",
    "            B, C, H, W = x_dummy.shape \n",
    "            del x_dummy\n",
    "            torch.mps.empty_cache()  # if using MPS\n",
    "            import gc; gc.collect()\n",
    "        self.fc = nn.Linear(C, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x) \n",
    "        x = self.conv2(x) \n",
    "        x = F.relu(x) \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.reshape(B, C, H*W)\n",
    "        x = x.permute(0, 2, 1)  # (B, H*W, C)\n",
    "        # x_embed = self.fc(x)  # (B, H*W, embed_size\n",
    "        B, N, C = x.shape\n",
    "        x_embed = self.fc(x)   # (B, N, embed_size) \n",
    "        x_embed = x_embed + self.pos_embed[:, :N, :].to(x_embed.device)  # add positional embedding\n",
    "        return x_embed  \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# caption_encoder = LSTMEncoder(embed_size, hidden_size, vocab_size)\n",
    "from transformers import GPT2LMHeadModel\n",
    "gpt_decoder = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt_decoder.resize_token_embeddings(gpt_decoder.get_input_embeddings().num_embeddings + 2)  # Example: add 3 tokens\n",
    "vocab_size = gpt_decoder.get_input_embeddings().num_embeddings\n",
    "\n",
    "\n",
    "\n",
    "gpt_hidden_size = gpt_decoder.config.hidden_size\n",
    "embed_size = gpt_hidden_size  # to match GPT2 hidden size\n",
    "hidden_size = gpt_hidden_size\n",
    "batch_size = 2\n",
    "input_channels = 3  \n",
    "image_h, image_w = 224, 224\n",
    "steps = 0\n",
    "epochs = 1\n",
    "lr = 1e-5\n",
    "accumulation_steps = 4  # simulate batch_size * 2\n",
    "\n",
    "train_dataset_cocooptions=DataLoaderLite(train_dataset_cocooptions, caption_length=50, tokenizer=tokenizer)           \n",
    "train_dataloader = DataLoader(train_dataset_cocooptions, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "total_steps = len(train_dataloader)  * epochs\n",
    "import math \n",
    "\n",
    "formatted_str = f\"Training details vocab size {vocab_size} batch size {batch_size} image size {image_h}x{image_w}\"\n",
    "formatted_str+= f\" total steps {total_steps} epochs {epochs}\"\n",
    "formatted_str+= f\"Max loss {math.log(vocab_size)}\"\n",
    "formatted_str+= f\"Perplexity {math.exp(math.log(vocab_size))}\"\n",
    "\n",
    "\n",
    "print (formatted_str)\n",
    "scaler = GradScaler()\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "import gc; gc.collect()\n",
    "\n",
    "#encoder_model = CNNEncoder(embed_size, [batch_size, input_channels, image_h, image_w])\n",
    "\n",
    "encoder_model = ResnetEncoder(embed_size)\n",
    "encoder_model = encoder_model.to(device)\n",
    "\n",
    "\n",
    "print (\"Loading caption model\")\n",
    "\n",
    "caption_encoder = ResnetGPT2Wrapper(gpt_decoder, embed_size, vocab_size)\n",
    "\n",
    "caption_encoder = caption_encoder.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_params = list([param for param in encoder_model.parameters() if param.requires_grad]) + list(caption_encoder.parameters())\n",
    "\n",
    "print (\"Trainable parameters in encoder model:\")\n",
    "print (sum(p.numel() for p in all_params if p.requires_grad))\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(all_params, lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps/accumulation_steps, eta_min=1e-6)\n",
    "\n",
    "import time \n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print (f\"Starting epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2] # [B, 3, 224, 224], [B, T], [B, T] \n",
    "        image_tensor, caption_tensor, attention_mask = image_tensor.to(device), caption_tensor.to(device), attention_mask.to(device)\n",
    "        B, C, H, W = image_tensor.shape\n",
    "\n",
    "        global_step = epoch * len(train_dataloader) + step + 1\n",
    "\n",
    "        print (f\"Step {step+1}/{len(train_dataloader)} Global step {global_step}/{total_steps}\")\n",
    "     \n",
    "\n",
    "        with torch.autocast(\"mps\", enabled=False):\n",
    "\n",
    "            print (\"Running encoder model\")\n",
    "            print(\"current allocated memory:\", torch.mps.current_allocated_memory() / 1e9, \"GB\")\n",
    "            print(\"driver allocated memory:\", torch.mps.driver_allocated_memory() / 1e9, \"GB\")\n",
    "\n",
    "            x_embed = encoder_model(image_tensor) # (B, N, embed_size) \n",
    "\n",
    "            print(\"current allocated memory:\", torch.mps.current_allocated_memory() / 1e9, \"GB\")\n",
    "            print(\"driver allocated memory:\", torch.mps.driver_allocated_memory() / 1e9, \"GB\")\n",
    "\n",
    "\n",
    "            #x_caption = caption_encoder(x_embed, caption_tensor)\n",
    "\n",
    "            print (\"Running caption model\")\n",
    "\n",
    "            logits = caption_encoder(x_embed, caption_tensor, attention_mask)  # (B, T-1, vocab_size)\n",
    "\n",
    "            B, T, C = logits.shape\n",
    "            preds = logits.reshape(B*T, C)\n",
    "            targets = caption_tensor[:, 1:].reshape(-1)\n",
    "            caption_loss = loss_fn(preds, targets)\n",
    "            loss = caption_loss / accumulation_steps  \n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        all_params = list([param for param in encoder_model.parameters() if param.requires_grad]) + list(caption_encoder.parameters())\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, max_norm=5.0)\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "          # estimate remaining time every 100 steps\n",
    "        if global_step % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            steps_per_sec = global_step / elapsed\n",
    "            remaining_steps = total_steps - global_step\n",
    "            est_remaining = remaining_steps / steps_per_sec\n",
    "            est_total = total_steps / steps_per_sec\n",
    "\n",
    "            print(f\"epoch {epoch+1}/{epochs} step {step}/{len(train_dataloader)} \"\n",
    "                  f\"Loss: {loss.item()*accumulation_steps:.4f} | \"\n",
    "                  f\"Elapsed: {elapsed/60:.2f} min | \"\n",
    "                  f\"ETA: {est_remaining/60:.2f} min | \"\n",
    "                  f\"Total est: {est_total/60:.2f} min | \"\n",
    "                  f\"Memory: {torch.mps.current_allocated_memory() / 1e9:.2f} GB , \\ {torch.mps.driver_allocated_memory() / 1e9:.2f} GB | \"\n",
    "                  f\"Perplexity {math.exp(loss.item()*accumulation_steps):.2f}\"\n",
    "                  )\n",
    "\n",
    "    if (step + 1) % accumulation_steps != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, 5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        # if step % 100 == 0:\n",
    "        #     print(f\" epoch {epoch+1}/{epochs} step {step}/{total_steps} Loss: {loss.item()}\")\n",
    "\n",
    "        if step % 1 == 0:\n",
    "            print(\"current allocated memory:\", torch.mps.current_allocated_memory() / 1e9, \"GB\")\n",
    "            print(\"driver allocated memory:\", torch.mps.driver_allocated_memory() / 1e9, \"GB\")\n",
    "\n",
    "\n",
    "    del image_tensor, caption_tensor, x_embed, logits, preds, targets\n",
    "    torch.mps.empty_cache()\n",
    "    import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed806315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "def generate_caption(encoder_model, caption_encoder, image_tensor, tokenizer, max_len=50, device=\"mps\", use_image=True):\n",
    "    caption_encoder.eval()\n",
    "    encoder_model.eval()\n",
    "\n",
    "    image_tensor = image_tensor.to(device).unsqueeze(0)  # (1, 3, 224, 224)\n",
    "\n",
    "    # Encode image only if use_image is True\n",
    "    if use_image:\n",
    "        img_features = encoder_model(image_tensor)  # (1, N, embed_size)\n",
    "    else:\n",
    "        img_features = torch.zeros((1, 196, embed_size), device=device)  # dummy zeros\n",
    "\n",
    "    # Start token\n",
    "    start_id = tokenizer.convert_tokens_to_ids(\"<START> \")\n",
    "    dummy_id = tokenizer.convert_tokens_to_ids(\"<PAD>\") \n",
    "    input_ids = torch.tensor([ [start_id, dummy_id] ], device=device)\n",
    "\n",
    "\n",
    "    generated_ids = input_ids\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            logits = caption_encoder(img_features, generated_ids)  # (1, T, vocab_size)\n",
    "            next_token_logits = logits[:, -1, :]  # last step\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "\n",
    "            if next_token_id.item() == tokenizer.convert_tokens_to_ids(\"<END>\"):\n",
    "                break\n",
    "\n",
    "    # Decode generated tokens\n",
    "    caption = tokenizer.decode(generated_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "counter = 0 \n",
    "# Example usage\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    image_tensor, _ = batch[0], batch[1]\n",
    "    #caption_without_image = generate_caption(encoder_model, caption_encoder, image_tensor[0], tokenizer, use_image=False)\n",
    "\n",
    "    caption_with_image = generate_caption(encoder_model, caption_encoder, image_tensor[0], tokenizer, use_image=True)\n",
    "\n",
    "    plt.imshow(image_tensor[0].permute(1,2,0).cpu().numpy())\n",
    "    print(\"With image context: \", caption_with_image)\n",
    "    # print(\"Without image context: \", caption_without_image)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "captions = torch.randn(1, 2, 4) \n",
    "y = torch.randn(1, 2, 4)\n",
    "captions, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0dd801",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((captions, y), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7159c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.concat((captions, y), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4747170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodel (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
